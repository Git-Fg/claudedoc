# L'Approche "Less is More" dans l'Optimisation des IA Agents : Guide Complet

## Philosophie Fondamentale : "Less is More"

### Les DÃ©couvertes d'Anthropic
Selon le rapport de recherche d'Anthropic ["Building Effective Agents"](https://www.anthropic.com/research/building-effective-agents), les systÃ¨mes d'agents IA les plus rÃ©ussis ne reposent **pas sur des frameworks complexes** mais sur des **modÃ¨les simples et composables**[citation:2][citation:20]. Leur Ã©tude avec des dizaines d'Ã©quipes industrielles a rÃ©vÃ©lÃ© que :

- **La simplicitÃ© prime sur la complexitÃ© technique**
- **Les abstractions superflues nuisent Ã  la performance**
- **Les systÃ¨mes minimalistes surpassent les architectures surchargÃ©es**

### Pourquoi la SimplicitÃ© Fonctionne Mieux

**1. Ã‰viter le "Context Poisoning" (Empoisonnement du Contexte)**
Lorsque vous surchargez le contexte avec trop d'informations, d'outils ou de structures complexes, vous crÃ©ez :
- **InterfÃ©rences contextuelles** : Le modÃ¨le surestime l'importance d'informations anciennes
- **Confusions contextuelles** : Trop d'outils induisent le modÃ¨le Ã  en utiliser inutilement
- **Conflits contextuels** : Informations contradictoires de diffÃ©rentes sources[citation:3]

**2. Le Principe de "NÃ©cessaire et Suffisant"**
```markdown
# Approche Complexe (Ã€ Ã‰viter)
- 15+ outils chargÃ©s simultanÃ©ment
- Prompts structurÃ©s de 50+ lignes
- HiÃ©rarchies d'agents multiples
- MCP servers surchargÃ©s

# Approche "Less is More" (RecommandÃ©e)
- 3-5 outils essentiels par tÃ¢che
- Prompts concis de 1-3 lignes quand possible
- Architecture plate quand cela suffit
- Chargement dynamique des outils uniquement si nÃ©cessaire
```

## IngÃ©nierie des Prompts : L'Art de la SimplicitÃ©

### Pourquoi les Prompts Simples Parfois Mieux Fonctionnent

**1. RÃ©duction de la Charge Cognitive**
Les modÃ¨les LLM ont une fenÃªtre contextuelle limitÃ©e. Les prompts simples :
- **Minimisent le bruit informationnel**
- **Ã‰vitent la dilution des instructions clÃ©s**
- **Permettent au modÃ¨le de se concentrer sur l'essentiel**[citation:15][citation:27]

**2. Ã‰viter la Sur-SpÃ©cification**
```javascript
// âŒ Trop complexe et contraignant
"Ã‰cris une fonction Python qui calcule la moyenne d'une liste de nombres, 
en utilisant une approche fonctionnelle avec des lambda, 
en gÃ©rant les cas d'erreur, avec une documentation complÃ¨te, 
en suivant PEP8, et en retournant None si la liste est vide"

// âœ… Simple et efficace  
"Calcule la moyenne d'une liste de nombres en Python"
```

**3. Le Pouvoir de la Confiance Implicite**
Les modÃ¨les modernes (Claude 3.5+, GPT-4.1+) comprennent mieux les intentions gÃ©nÃ©rales que les instructions ultra-spÃ©cifiques qui peuvent les "bloquer" dans des interprÃ©tations trop littÃ©rales[citation:26].

### Techniques de Prompting Minimalistes Efficaces

**1. Prompting Direct (Quand C'est Possible)**
```bash
# Au lieu de :
"En tant que dÃ©veloppeur senior expert, analyse ce code et suggÃ¨re des amÃ©liorations..."

# PrÃ©fÃ¨re :
"Analyse ce code et suggÃ¨re 3 amÃ©liorations concrÃ¨tes"
```

**2. Utilisation StratÃ©gique de DÃ©limiteurs**
```markdown
<task>Corrige le bug dans la fonction login()</task>
<constraints>Ne modifie pas la signature de la fonction</constraints>
<code>[code Ã  analyser]</code>
```

**3. Ã‰limination des "Faux Amis"**
Supprimez systÃ©matiquement :
- Les "Vous Ãªtes un expert..." superflus
- Les explications dÃ©taillÃ©es quand une phrase suffit
- Les contraintes redondantes

## CrÃ©ation de Skills et d'Agents : L'Approche Minimaliste

### Principes de Base

**1. Single Responsibility Principle (SRP) pour les Agents**
Chaque agent/skill doit avoir **une seule responsabilitÃ© claire** :
- âŒ Agent "tout-en-un" qui fait le cafÃ© et dÃ©bugge
- âœ… Agent "Debug" + Agent "Refactoring" + Agent "Tests"

**2. Chargement Dynamique des Outils**
```typescript
// âŒ Mauvaise pratique : Chargement statique de 20+ outils
const tools = [tool1, tool2, ..., tool20];

// âœ… Bonne pratique : Chargement dynamique basÃ© sur la tÃ¢che
const relevantTools = await selectToolsForTask(userQuery); // 3-5 outils max
```

**3. Architecture "Plan-Build"**
InspirÃ© d'OpenCode, adoptez une approche Ã  deux modes :
- **Mode Plan** : Analyse et planification (lecture seule)
- **Mode Build** : ExÃ©cution et modification (Ã©criture)[citation:7][citation:31]

### Optimisation des Performances

**1. Gestion Contextuelle Ã‰conome**
```javascript
// ImplÃ©mentez un systÃ¨me de "context pruning"
function pruneContext(context) {
  return context
    .filter(msg => msg.role === 'user' || msg.role === 'assistant')
    .slice(-5); // Garde seulement les 5 derniers Ã©changes
}
```

**2. Routage Intelligent (Anthropic Style)**
Inspirez-vous de la classification d'Anthropic :
- **Simple** : Questions factuelles directes
- **Reasoning** : NÃ©cessite du raisonnement multi-Ã©tapes  
- **Internet Search** : Besoin d'informations actuelles[citation:1]

## Choix des Outils : Claude Code vs OpenCode

### Comparaison DÃ©taillÃ©e

| CritÃ¨re | Claude Code | OpenCode |
|---------|-------------|----------|
| **ModÃ¨les SupportÃ©s** | Claude uniquement | 75+ fournisseurs (GPT, Claude, Gemini, GLM, etc.) |
| **CoÃ»t** | $20-50/mois | Gratuit (payez seulement les API utilisÃ©es) |
| **Personnalisation** | LimitÃ©e | IllimitÃ©e (code source ouvert) |
| **Performance** | Plus rapide (optimisÃ©) | LÃ©gÃ¨rement plus lent mais plus flexible |
| **CommunautÃ©** | Support officiel | 50K+ stars, 650+ contributeurs |
| **MCP Support** | Oui (propriÃ©taire) | Oui (standard ouvert) |

### Recommandations StratÃ©giques

**Choisissez Claude Code si :**
- Vous avez dÃ©jÃ  un abonnement Claude Pro/Max
- Vous recherchez une expÃ©rience "polie" et sans configuration
- La vitesse brute est votre prioritÃ© absolue
- Vous travaillez dans un environnement d'entreprise avec support officiel[citation:5]

**Choisissez OpenCode si :**
- Vous voulez Ã©viter le "vendor lock-in"
- Vous avez besoin de flexibilitÃ© de modÃ¨les
- Le coÃ»t est un facteur critique
- Vous souhaitez personnaliser et Ã©tendre les fonctionnalitÃ©s
- La confidentialitÃ© des donnÃ©es est primordiale[citation:7][citation:29]

### Approche Hybride Optimale

Pour une performance maximale avec simplicitÃ© :
```bash
# Utilisez OpenCode comme interface principale
# Avec chargement sÃ©lectif de modÃ¨les :

1. TÃ¢ches simples â†’ GLM-4.7 (gratuit)
2. Code complexe â†’ Claude Sonnet 4.5 (via OpenCode)
3. Recherche â†’ Google Gemini 2.5 Pro
4. Tests â†’ ModÃ¨le local (Ollama)
```

## Bonnes Pratiques pour une Performance Optimale

### 1. Workflow Minimaliste

```mermaid
graph TD
    A[Utilisateur] --> B[TÃ¢che Simple?]
    B -->|Oui| C[Prompt Direct + ModÃ¨le LÃ©ger]
    B -->|Non| D[Analyse par Agent SpÃ©cialisÃ©]
    D --> E[DÃ©composition en Sous-tÃ¢ches]
    E --> F[Agents ParallÃ¨les Simples]
    F --> G[SynthÃ¨se des RÃ©sultats]
```

### 2. Checklist d'Optimisation

**Avant d'ajouter de la complexitÃ©, vÃ©rifiez :**
- [ ] La tÃ¢che peut-elle Ãªtre rÃ©solue avec un prompt simple ?
- [ ] Ai-je besoin vraiment d'un agent ou d'un workflow suffit ?
- [ ] Chaque outil est-il utilisÃ© au moins 80% du temps ?
- [ ] Le contexte dÃ©passe-il 50% de la fenÃªtre du modÃ¨le ?
- [ ] Ai-je testÃ© la version simplifiÃ©e avant l'optimisation ?

### 3. Mesure de Performance

**Indicateurs ClÃ©s Ã  Suivre :**
- **Taux de succÃ¨s Ã  la premiÃ¨re tentative** (idÃ©al > 70%)
- **Nombre moyen d'itÃ©rations** (cible < 2)
- **CoÃ»t token par tÃ¢che** (optimiser vers le bas)
- **Temps de rÃ©ponse** (Ã©quilibre qualitÃ©/vitesse)

## Conclusion : L'Art du "Less is More"

L'optimisation des IA Agents n'est pas une course Ã  la complexitÃ© technique, mais une quÃªte de **simplicitÃ© Ã©lÃ©gante**. Comme le rappelle Anthropic dans leurs recherches : "The most effective implementations were not the most complex, but the simplest that could solve the problem"[citation:2].

**Votre nouvelle philosophie devrait Ãªtre :**
1. **Commencez simple** (prompt direct)
2. **Complexifiez uniquement si nÃ©cessaire** (ajoutez un agent)
3. **Mesurez avant d'optimiser** (donnÃ©es > opinions)
4. **PrivilÃ©giez la composition Ã  la complexitÃ©** (skills rÃ©utilisables)

En adoptant cette approche minimaliste mais stratÃ©gique, vous obtiendrez des systÃ¨mes d'agents plus performants, plus fiables et plus Ã©conomiques - quel que soit l'outil choisi (Claude Code ou OpenCode).


---


## Le Paradoxe "Less is More" dans l'Optimisation des AI Agents

Votre intuition est correcte : aprÃ¨s avoir maÃ®trisÃ© les primitives techniques, le vÃ©ritable gain de performance vient souvent de la simplification radicale. La communautÃ© converge vers un consensus frappant : l'over-engineering est le poison des AI agents. [reddit](https://www.reddit.com/r/AI_Agents/comments/1ph5m9c/are_we_overengineering_agents_when_simple_systems/)

## Le ProblÃ¨me de Surcharge des Outils (Tool Overload)

### Impact Dramatique sur les Performances

Les recherches rÃ©vÃ¨lent des chiffres alarmants sur le coÃ»t de la complexitÃ©: [joshuaberkowitz](https://joshuaberkowitz.us/blog/news-1/solving-tool-overload-in-ai-agents-with-semantic-selection-1735)

- **PrÃ©cision catastrophique** : La prÃ©cision des modÃ¨les chute de **78% avec 10 outils Ã  seulement 13% avec plus de 100 outils**
- **Ã‰chec de sÃ©lection** : Les modÃ¨les de retrieval state-of-the-art atteignent moins de 35% de prÃ©cision dans la sÃ©lection d'outils
- **Baisse de succÃ¨s** : Une mauvaise qualitÃ© de retrieval rÃ©duit directement les taux de succÃ¨s des tÃ¢ches de 10-20%

### Le Consensus Communautaire

Un thread Reddit sur l'over-engineering des agents (92 votes) capture le sentiment gÃ©nÃ©ral: [reddit](https://www.reddit.com/r/AI_Agents/comments/1ph5m9c/are_we_overengineering_agents_when_simple_systems/)

> "From our own experience, over-engineering is poison for AI agents. Greater stability arises from minimizing the number of components rather than complicating them. Complexity often undermines consistent behavior."

Un dÃ©veloppeur rÃ©sume : "We concentrated on maximum autonomy with minimal constraints, moving away from ReAct. The end product operates on simple text commands, with core code comprising roughly 2,000 lines". [reddit](https://www.reddit.com/r/AI_Agents/comments/1ph5m9c/are_we_overengineering_agents_when_simple_systems/)

## Pourquoi les Prompts Simples Surpassent les Complexes

### Les MÃ©canismes Sous-jacents

**1. Context Window Bloat**

Chaque outil, instruction dÃ©taillÃ©e, ou structure complexe mange du contexte prÃ©cieux: [youtube](https://www.youtube.com/watch?v=eOMq4suzl3U)
- Plus de tokens = moins d'espace pour le raisonnement rÃ©el
- Les prompts longs dÃ©gradent la performance Ã  cause des limites de contexte
- Le token usage par requÃªte peut exploser de 1,084 Ã  127,315 tokens avec trop d'outils [joshuaberkowitz](https://joshuaberkowitz.us/blog/news-1/solving-tool-overload-in-ai-agents-with-semantic-selection-1735)

**2. Decision Fatigue**

Chaque choix supplÃ©mentaire augmente la charge cognitive du modÃ¨le: [curam-ai.com](https://curam-ai.com.au/the-role-of-ai-agents-efficiency-through-minimalism-and-orchestration/)
- Plus d'options = temps d'exÃ©cution plus longs
- Choix inconsistants dus Ã  la surcharge dÃ©cisionnelle
- La latence augmente avec chaque test d'outil avant sÃ©lection

**3. Clarity Over Structure**

Un dÃ©veloppeur expÃ©rimentÃ© note: [linkedin](https://www.linkedin.com/posts/richard-oleson-408784101_working-with-engineering-prompts-for-llm-activity-7386060970370060288-CcAU)
> "Concise prompts with contextual relevance often outperform lengthy context-heavy prompts. Detailed prompts suit specific workflows with predefined outcomes, but simplicity is key when exploring possibilities."

## Principes d'Optimisation pour MCP Servers

### Les RÃ¨gles d'Or de l'EfficacitÃ© MCP

Une prÃ©sentation technique sur l'efficacitÃ© des serveurs MCP en production rÃ©vÃ¨le les patterns critiques: [youtube](https://www.youtube.com/watch?v=eOMq4suzl3U)

**MÃ©triques prioritaires :**
- Task completion rate (mÃ©trique nord)
- Tool selection accuracy
- Parameter accuracy
- Token output (coÃ»t)
- Interaction count (latence)

**Principes de design :**
1. **Fewer tools + richer parameters** : PrÃ©fÃ©rer moins d'outils avec plus de paramÃ¨tres polymorphiques
2. **Tools as "agent stories"** : Traiter les outils comme des histoires packagÃ©es, pas des wrappers API 1:1
3. **Trim response payloads** : RÃ©duire les rÃ©ponses JSON, supprimer le redondant
4. **Actionable errors** : Erreurs qui aident les modÃ¨les Ã  s'auto-corriger
5. **Stable tool lists** : Ne pas changer la liste d'outils mid-session (prÃ©serve le cache) [youtube](https://www.youtube.com/watch?v=eOMq4suzl3U)

> "Less is better because the less tokens you put out, the less context window you're eating up and the better chances you have for the model to reach task completion." [youtube](https://www.youtube.com/watch?v=eOMq4suzl3U)

## Optimisation de Claude Code : RÃ©sultats Concrets

### Le Pouvoir du Prompt Learning

Arize AI a dÃ©montrÃ© qu'optimiser **uniquement le system prompt** de Claude Code (sans changement d'architecture, d'outils, ou de fine-tuning) produit: [arize](https://arize.com/blog/claude-md-best-practices-learned-from-optimizing-claude-code-with-prompt-learning/)

- **+10% boost sur SWE Bench** avec des optimisations ciblÃ©es
- **+5% gains en performance gÃ©nÃ©rale** de coding
- **Gains encore plus importants** quand spÃ©cialisÃ© sur un repo unique

### Best Practices Claude Code

Pour les performances optimales: [claude](https://claude.com/blog/optimize-code-performance-quickly)

**Configuration tempÃ©rature :**
- **0.0-0.2** : TrÃ¨s focalisÃ© et dÃ©terministe, idÃ©al pour analyse de code et planification
- **0.3-0.5** : Ã‰quilibrÃ© avec crÃ©ativitÃ©, bon pour tÃ¢ches de dÃ©veloppement gÃ©nÃ©rales
- **0.6-1.0** : Plus crÃ©atif, utile pour brainstorming

**Focus sur les critical paths :**
- Lancer Claude Code dans les rÃ©pertoires critiques (`api/`, `core/`)
- Ã‰viter l'analyse d'assets statiques ou fichiers de config
- Appliquer des patterns systÃ©matiques pour identifier les inefficacitÃ©s rÃ©currentes

## OpenCode : Configuration Minimaliste et Efficace

### Architecture et Philosophie

OpenCode adopte une approche client/serveur open-source avec focus sur la simplicitÃ©: [opencode](https://opencode.ai)

**Configuration agents optimale :**
```
description: Brief description (required)
temperature: Response randomness (0.0-1.0)
maxSteps: Limit agentic iterations
model: Override global model
tools: Enable/disable tools (minimal set)
permission: Tool access control
mode: primary, subagent, or all
```

**Principe clÃ© :** Les agents peuvent facilement Ãªtre configurÃ©s pour des tÃ¢ches spÃ©cialisÃ©es, mais la vraie performance vient de la limitation des steps et du toolkit. [opencode](https://opencode.ai/docs/agents/)

## Implications Pratiques pour le Prompt Engineering

### Le ModÃ¨le "Start Small, Expand Gradually"

L'approche recommandÃ©e par la communautÃ©: [sparkco](https://sparkco.ai/blog/mastering-agent-prompt-engineering-a-2025-deep-dive)

1. **Commencer avec un paragraphe simple** dÃ©crivant l'intention claire
2. **Tester et observer** les rÃ©sultats
3. **Ajouter de la complexitÃ© uniquement si nÃ©cessaire** pour des cas spÃ©cifiques
4. **Ã‰viter la structure prÃ©emptive** : ne pas anticiper tous les edge cases

### Quand Utiliser la ComplexitÃ©

La complexitÃ© structurÃ©e reste pertinente pour: [hyphadev](https://www.hyphadev.io/blog/ai-prompt-optimizer)

- **Workflows rÃ©pÃ©tables** avec outputs prÃ©dÃ©finis
- **IntÃ©gration systÃ¨me** (CRMs, support platforms)
- **Production critique** oÃ¹ la consistance est essentielle
- **Tasks nÃ©cessitant chain-of-thought explicite**

**Mais attention** : Le Prompt Improver de Claude crÃ©e des templates qui produisent des rÃ©ponses plus longues, plus dÃ©taillÃ©es, mais **plus lentes**. Ã€ utiliser uniquement quand la prÃ©cision prime sur la latence. [anthropic.mintlify](https://anthropic.mintlify.app/en/docs/build-with-claude/prompt-engineering/prompt-improver)

## Toolkit Focus : Approche Optimale

### Principes de SÃ©lection d'Outils

**La rÃ¨gle des 10 outils maximum**: [linkedin](https://www.linkedin.com/posts/raphaelmansuy_benchmarking-tool-retrieval-for-large-language-activity-7302570464206282752-EIlu)
- Au-delÃ  de 10 outils, la prÃ©cision commence Ã  dÃ©cliner significativement
- PrivilÃ©gier des outils polymorphiques riches en paramÃ¨tres
- Utiliser semantic tool selection pour les catalogues de plus de 50 outils

**HiÃ©rarchie recommandÃ©e :**
1. **Outils essentiels** : 3-5 outils de base couvrant 80% des use cases
2. **Outils spÃ©cialisÃ©s** : 2-3 outils pour domaines spÃ©cifiques
3. **Outils contextuels** : ChargÃ©s dynamiquement selon le besoin

### Stack Technique Optimal

Pour Claude Code et OpenCode: [claude](https://claude.com/blog/optimize-code-performance-quickly)

**Core minimal :**
- SystÃ¨me de fichiers (read/write/search)
- ExÃ©cution de commandes
- Git operations basiques
- Documentation lookup

**Extensions conditionnelles :**
- Database tools (uniquement si pertinent)
- API testing (pour projets backend)
- Deployment tools (pour DevOps)

## Optimisation du Codebase pour Agents

Un dÃ©veloppeur partage des rÃ©sultats impressionnants aprÃ¨s simplification: [dev](https://dev.to/aarongustafson/optimizing-your-codebase-for-ai-coding-agents-4ndm)

**RÃ©sultats mesurÃ©s :**
- **~40% rÃ©duction du temps de traitement**
- **~75% rÃ©duction de l'usage de tokens**
- **>80% rÃ©duction de confusion et raisonnement circulaire**

**MÃ©thodes appliquÃ©es :**
- Documentation consolidÃ©e et facile Ã  naviguer
- Scripts de validation rapides
- Gestion explicite des edge cases
- Plans dÃ©taillÃ©s de structure de module

> "These improvements don't just help the AI agent. They help everyone." [dev](https://dev.to/aarongustafson/optimizing-your-codebase-for-ai-coding-agents-4ndm)

## SynthÃ¨se : Le Framework "Less is More"

### Pour le Prompt Engineering

**âŒ Ã‰viter :**
- Prompts de plusieurs pages avec toutes les rÃ¨gles possibles
- Structures XML/JSON complexes sans raison claire
- Instructions contradictoires ou redondantes
- Exemples exhaustifs couvrant tous les edge cases

**âœ… PrivilÃ©gier :**
- Intent clair en 1-3 phrases
- Contexte minimal mais pertinent
- Instructions actionables et non ambiguÃ«s
- Expansion itÃ©rative basÃ©e sur les rÃ©sultats

### Pour les Skills et Agents

**âŒ Ã‰viter :**
- CrÃ©er des skills pour chaque micro-tÃ¢che
- Subagents imbriquÃ©s sur plusieurs niveaux
- Hooks complexes avec multiples conditions
- Duplication de fonctionnalitÃ©s entre tools

**âœ… PrivilÃ©gier :**
- Skills polymorphiques couvrant des familles de tÃ¢ches
- Architecture plate avec agents spÃ©cialisÃ©s
- Hooks minimalistes avec logique claire
- Composition par paramÃ¨tres plutÃ´t que par multiplication

### Pour les MCP Servers

**âŒ Ã‰viter :**
- Exposer toutes les fonctions API comme outils sÃ©parÃ©s
- Payloads JSON verbeux avec mÃ©tadonnÃ©es inutiles
- Changements frÃ©quents de la liste d'outils
- Erreurs gÃ©nÃ©riques sans guidance

**âœ… PrivilÃ©gier :**
- 5-10 outils maximum par serveur
- RÃ©ponses trimÃ©es au strict nÃ©cessaire
- Liste d'outils stable (cache-friendly)
- Erreurs actionnables suggÃ©rant le prochain outil

## La Nouvelle Approche de DÃ©veloppement

Le consensus Ã©mergent suggÃ¨re un workflow radicalement diffÃ©rent: [linkedin](https://www.linkedin.com/posts/richard-oleson-408784101_working-with-engineering-prompts-for-llm-activity-7386060970370060288-CcAU)

1. **DÃ©sapprendre la sophistication technique** : La maÃ®trise des primitives est utile, mais ne doit pas conduire Ã  les utiliser systÃ©matiquement

2. **Metrics-driven simplification** : Mesurer task completion rate, pas la sophistication architecturale

3. **Iterative minimalism** : Commencer nu, ajouter uniquement ce qui amÃ©liore mesurablÃ©ment les performances

4. **Documentation over configuration** : Un bon README vaut mieux que 50 skills parfaitement structurÃ©s

Cette approche reflÃ¨te un principe fondamental du dÃ©veloppement logiciel : **la meilleure abstraction est celle qu'on n'a pas eu besoin de crÃ©er**. [curam-ai.com](https://curam-ai.com.au/the-role-of-ai-agents-efficiency-through-minimalism-and-orchestration/)


---


# Optimizing AI Agent Performance: Less is More - glm-4.7
*Mode: advanced-search*

<metadata>
Messages: 2 | Tokens: 44,818
Outils: find, open, search
</metadata>

--------------------------------------------------

## User
J'ai dÃ©pensÃ© baucoup de temps Ã  essayer de comprendre les primitives et crÃ©er une documentation fiable et fondamentale autour de claude code / opencode / antigravity.
AprÃ¨s avoir appris fondamentalement Ã  propos des prompts, skills, subagents, tools, serveurs mcp, hooks ... je me rends compte qu'il est maintenant temps de dÃ©sapprendre et viser les performances au lieu de la technicitÃ©, et recentrer l'approche sur "Less is More". Effectue des recherches approfondies et dÃ©taillÃ©es sur l'ensemble des sentiments partagÃ©s ou non de la communautÃ©, et comment optimiser la performance des IA Agents en gÃ©nÃ©rale ainsi que dans le domaine du code en approchant cela. Qu'est-ce que Ã§a implique dans le prompt engineering, crÃ©ation de Skills et d'Agents ; et pourquoi parfois un simple paragraphe/ligne de texte donne de biens meilleurs rÃ©sultats qu'un prompting avancÃ© et optimisÃ©/structurÃ©. Quel toolkit focus et comment procÃ©der de maniÃ¨re optimale, simple et efficace, notamment avec des outils comme openccode et claude code. 
N'assume rien, et recherche tout.

## Assistant
<reasoning_trace>
1. Lâ€™utilisateur demande une analyse approfondie, fondÃ©e sur des sources externes, sur lâ€™optimisation des performances des agents IA (en gÃ©nÃ©ral et pour le code), pourquoi des prompts simples sont parfois plus efficaces que des prompts complexes, et comment appliquer ce "less is more" Ã  la crÃ©ation de skills/agents et Ã  lâ€™utilisation dâ€™outils comme Claude Code/OpenCode/opencode/Antigravity. Je vais chercher : (1) des retours de communautÃ© sur Claude Code, MCP, OpenCode/Antigravity et best pratiques dâ€™usage ; (2) la littÃ©rature sur le prompt engineering et lâ€™efficacitÃ© des prompts simples vs complexes ; (3) des guides architecturaux pour agents/assistants de code ; (4) des retours dâ€™expÃ©rience sur la surâ€‘spÃ©cification de prompts et le besoin de simplicitÃ©.
2. Je vais ouvrir lâ€™article Anthropic sur les best practices de Claude Code pour voir quelles recommandations officielles existent sur lâ€™utilisation de Claude Code en mode "agentic", notamment en termes de simplicitÃ© de prompting et de configuration, afin de les mettre en regard avec la philosophie "less is more".
3. Je vais descendre dans la page des best practices pour voir sâ€™il y a une section spÃ©cifique qui donne des recommandations concrÃ¨tes dâ€™utilisation (style de prompting, taille des prompts, utilisation dâ€™agents simples vs complexes, organisation des tÃ¢ches), afin de les appuyer dans mon analyse.
4. Je vais ouvrir lâ€™article Medium "I Studied 1500 Academic Papers on Prompt Engineering" pour extraire les rÃ©sultats concrets concernant la longueur et la complexitÃ© des prompts (notamment le fait que des prompts bien structurÃ©s mais courts surpassent souvent des prompts verbeux), afin dâ€™appuyer la partie thÃ©orique sur "less is more" dans le prompt engineering.
5. Le lecteur est restreint sur cette page Medium. Je vais chercher dâ€™autres articles publics ou billets de blog (non paywall) qui rÃ©sument des findings acadÃ©miques sur la performance relative de prompts simples vs complexes, ainsi que des guides sur les agents de code et la simplicitÃ© des prompts.
6. Je vais ouvrir (1) la page de documentation sur MCP dans Claude Code, (2) le site officiel dâ€™OpenCode pour comprendre son positionnement, (3) la page Ã©cosystÃ¨me/plugins/agents dâ€™OpenCode, et (4) un article sur les agents et prompting comme lâ€™article de Medium "The Art of Agent Prompting", afin dâ€™identifier comment la communautÃ© et les docs elles-mÃªmes recommandent des approches simples ou structurÃ©es pour skills/agents, et comment les outils sont pensÃ©s en termes de simplicitÃ© vs complexitÃ©.
7. Je vais ouvrir lâ€™article de blog PromptLayer sur "Why LLMs Get Distracted and How to Write Shorter Prompts" pour rÃ©cupÃ©rer les arguments techniques sur lâ€™attention du modÃ¨le, les distracteurs et les effets de la longueur/structure du prompt sur la performance, afin dâ€™expliquer pourquoi des prompts courts et denses sont souvent plus efficaces.
8. Je vais ouvrir quelques sources communautaires : un fil Reddit sur lâ€™excÃ¨s de complexitÃ© en prompt engineering, un post LinkedIn Ã  propos de Claude Code et du "less prompting", ainsi quâ€™un post de test sur Antigravity IDE. Cela me donnera des retours empiriques (positifs et nÃ©gatifs) sur la simplicitÃ© vs complexitÃ© et sur la stabilitÃ© dâ€™outils comme Antigravity, pour enrichir la section sur les sentiments de la communautÃ© et les leÃ§ons pratiques.
9. Je veux maintenant prÃ©ciser ce qui se dit dans la communautÃ© sur les subagents/skills/agents dans Claude Code et OpenCode, en particulier les retours sur la surâ€‘complexitÃ© et lâ€™intÃ©rÃªt de garder des agents trÃ¨s spÃ©cialisÃ©s et simples. Je vais chercher des blogs, fils de discussion et docs qui abordent explicitement "less is more" ou la simplicitÃ© des agents/subagents/skills.
10. Je vais ouvrir (1) lâ€™article Anthropic sur lâ€™ingÃ©nierie de contexte pour agents, (2) la doc officielle des best practices pour les Skills Claude, (3) la doc des Agent Skills OpenCode, et (4) le guide Medium "Writing OpenCode Agent Skills". Cela me permettra de voir comment les Ã©quipes officielles et la communautÃ© dÃ©crivent les bonnes pratiques de crÃ©ation de skills/agents et si elles insistent sur la simplicitÃ©, la compaction et lâ€™itÃ©ration sur des tÃ¢ches ciblÃ©es plutÃ´t que des mega-prompts complexes.
11. Pour complÃ©ter la partie sur les agents de code en gÃ©nÃ©ral, je vais chercher des recommandations de Google/Antigravity et dâ€™autres acteurs (Cursor, etc.) sur la simplicitÃ© des prompts et des workflows agents, ainsi que des Ã©tudes/bilans empiriques sur lâ€™efficacitÃ© des assistants de code type Cursor/Copilot pour connecter cela avec la notion de "less is more".
12. Je vais ouvrir (1) les best practices de GitHub Copilot pour le prompting et (2) les best practices pour lâ€™usage de Copilot en gÃ©nÃ©ral, pour voir dans quelle mesure ils recommandent aussi la simplicitÃ©, la progressivitÃ© (du gÃ©nÃ©ral au spÃ©cifique) et la gestion fine du contexte plutÃ´t que des prompts ultra-structurÃ©s et lourds.
13. Je cherche dans la doc Copilot la section qui recommande de "commencer gÃ©nÃ©ral puis devenir spÃ©cifique" dans les prompts, car câ€™est une bonne illustration dâ€™une approche simple et progressive plutÃ´t que de tout spÃ©cifier dâ€™un coup dans un mÃ©ga-prompt structurÃ©.
14. Jâ€™ouvre la section prÃ©cise de la doc Copilot oÃ¹ ils expliquent "Start general, then get specific" afin de citer cette recommandation en exemple de bonne pratique simple et progressive pour le prompt appliquÃ© au code.
---
15. Maintenant, laissez-moi rÃ©pondre Ã  la question de l'utilisateur.
</reasoning_trace>

[ğŸ” Search]

[ğŸŒ Opened: [Claude Code: Best practices for agentic coding](https://www.anthropic.com/engineering/claude-code-best-practices)]

[ğŸŒ Opened: [Claude Code: Best practices for agentic coding](https://www.anthropic.com/engineering/claude-code-best-practices)]

[ğŸŒ Opened: [I Studied 1500 Academic Papers on Prompt Engineering](https://aakashgupta.medium.com/i-studied-1-500-academic-papers-on-prompt-engineering-heres-why-everything-you-know-is-wrong-2fc97c1735a7)]

[ğŸ” Search]

[ğŸŒ Opened: [Connect Claude Code to tools via MCP](https://code.claude.com/docs/en/mcp)]

[ğŸŒ Opened: [Why LLMs Get Distracted and How to Write Shorter Prompts](https://blog.promptlayer.com/why-llms-get-distracted-and-how-to-write-shorter-prompts)]

[ğŸŒ Opened: [Anyone else think prompt engineering is getting way too](https://www.reddit.com/r/PromptEngineering/comments/1nicu36/anyone_else_think_prompt_engineering_is_getting)]

[ğŸ” Search]

[ğŸŒ Opened: [Effective context engineering for AI agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)]

[ğŸ” Search]

[ğŸŒ Opened: [Prompt engineering for GitHub Copilot Chat](https://docs.github.com/en/copilot/concepts/prompting/prompt-engineering)]

[ğŸ” Find]

[ğŸŒ Opened: [Prompt engineering for GitHub Copilot Chat](https://docs.github.com/en/copilot/concepts/prompting/prompt-engineering)]

Super question, et super lÃ  oÃ¹ tu en es dans ta courbe dâ€™apprentissage.

Je vais rÃ©pondre en 5 temps:
1) Ce que dit vraiment la communautÃ© et la recherche aujourdâ€™hui.
2) Pourquoi un simple paragraphe bat souvent un mega-prompt structurÃ©.
3) Ce que Ã§a change concrÃ¨tement dans le prompt engineering.
4) Ce que Ã§a change pour les Skills et les Agents.
5) Un â€œtoolkit focusâ€ et un mode opÃ©ratoire simple pour Claude Code + OpenCode (Antigravity en bonus).

---

## 1. Ce que dit la communautÃ© et la recherche aujourdâ€™hui

**Gros message:** le vent a bien tournÃ©. On est passÃ©s du â€œplus tu prompts, mieux câ€™estâ€ Ã  un â€œless is moreâ€ assez net, autant dans la recherche que dans les retours dâ€™usage.

Quelques points clÃ©s:

- Sur les prompts:
  - Une synthÃ¨se de 1500+ papiers sur le prompt engineering conclut que les prompts courts, bien structurÃ©s et denses surpassent souvent les prompts verbeux, pour une qualitÃ© Ã©gale ou meilleure et un coÃ»t bien moindreã€turn4search0ã€‘.
  - Des travaux acadÃ©miques rÃ©cents montrent que pour les modÃ¨les performants, des prompts simples (parfois zero-shot) peuvent outperformer des styles plus complexes en coÃ»t/complexitÃ©ã€turn4search1ã€‘.
  - Un article de PromptLayer qui rÃ©sume lâ€™Ã©tude Chroma â€œContext Rotâ€ montre que tous les gros modÃ¨les (y compris GPTâ€‘4.1, Claude 4, etc.) dÃ©gradent en prÃ©cision quand le contexte devient trop long: lâ€™idÃ©e de â€œplus de contexte = meilleur rÃ©sultatâ€ est un mytheã€turn6fetch0ã€‘.
  - Chroma trouve par exemple que ajouter tout lâ€™historique de chat (~113k tokens) peut faire chuter lâ€™accuracy dâ€™environ 30% par rapport Ã  une version ciblÃ©e de ~300 tokensã€turn6fetch0ã€‘.

- Sur les agents & outils de code:
  - Plusieurs posts LinkedIn (largement relayÃ©s) expliquent que â€œover-engineering is killing your AI agentsâ€ et que la leÃ§on principale de Claude Code, câ€™est de garder une boucle unique simple, avec une todo-list et un petit nombre de modÃ¨les pour 70â€“80% des tÃ¢chesã€turn7fetch1ã€‘ã€turn8search14ã€‘ã€turn8search15ã€‘ã€turn8search16ã€‘.
  - Anthropic publie â€œEffective context engineering for AI agentsâ€ oÃ¹ ils expliquent que lâ€™enjeu pour les agents nâ€™est plus de trouver les â€œbons motsâ€ dans le prompt, mais de concevoir la plus petite configuration de contexte haute-signal possible (moins dâ€™information, mais plus pertinente)ã€turn9fetch0ã€‘.
  - Des devs expliquent avoir crÃ©Ã© des sub-agents Claude Code ciblÃ©s (par ex. pour traiter des opÃ©rations lourdes dans un contexte sÃ©parÃ©) afin de rÃ©duire la â€œcontext pollutionâ€ de 95â€“98% et de renvoyer uniquement le rÃ©sultat condensÃ© Ã  lâ€™agent principalã€turn8search0ã€‘.
  - Des docs officielles pour Claude Skills et OpenCode Skills insistent toutes sur â€œconcise is keyâ€: on suppose que le modÃ¨le est dÃ©jÃ  trÃ¨s compÃ©tent, et chaque token doit se justifierã€turn9fetch1ã€‘ã€turn9fetch2ã€‘.
  - GitHub Copilot recommande dans sa doc de â€œcommencer gÃ©nÃ©ral puis devenir spÃ©cifiqueâ€ et de â€œcasser les tÃ¢ches complexes en tÃ¢ches simplesâ€ plutÃ´t que de tout balader dans un prompt ultra-dÃ©taillÃ©ã€turn13fetch0ã€‘.

- Sur la performance rÃ©elle des assistants de code:
  - Une Ã©tude sur Cursor (assistant de code agent) montre un gain significatif en vÃ©locitÃ© et qualitÃ© sur les projets qui lâ€™adoptent par rapport Ã  des projets similairesã€turn0search8ã€‘.
  - Dâ€™autres RCT trouvent parfois lâ€™inverse: des devs expÃ©rimentÃ©s mettent 19% de temps en plus avec lâ€™IA que sansã€turn10search15ã€‘ã€turn10search16ã€‘. Ã‡a confirme que la faÃ§on dâ€™utiliser les agents (complexitÃ©, sur-automation, perte de contrÃ´le) change radicalement le rÃ©sultat: tout nâ€™est pas question dâ€™outil, mais de workflow.
  - Des rapports comme le â€œAI Productivity Paradoxâ€ montrent que les Ã©quipes qui tirent le plus de ces outils sont celles qui les utilisent de maniÃ¨re ciblÃ©e (MVP, automations rÃ©pÃ©titives), pas comme un â€œauto-pilotâ€ omniprÃ©sentã€turn10search18ã€‘ã€turn10search19ã€‘.

En rÃ©sumÃ©: la recherche + la pratique convergent vers:
- des prompts plus courts et plus denses,
- moins de couches dâ€™agents/skills inutiles,
- plus dâ€™attention Ã  ce quâ€™on met dans le contexte (et Ã  ce quâ€™on en retire).

---

## 2. Pourquoi un simple paragraphe/ligne bat souvent un prompt ultra-structurÃ©

Il y a plusieurs mÃ©canismes, qui se combinent:

### 1) Budget dâ€™attention fini

- Anthropic explique que les LLM ont un budget dâ€™attention limitÃ©: plus tu introduis de tokens, plus ce budget sâ€™Ã©puise et plus la capacitÃ© du modÃ¨le Ã  se concentrer sur ce qui compte dÃ©gradeã€turn9fetch0ã€‘.
- PromptLayer/Chroma montrent que les performances chutent Ã  cause du â€œcontext rotâ€ sur tous les gros modÃ¨les testÃ©s quand la longueur de contexte augmenteã€turn6fetch0ã€‘.

RÃ©sultat: un mega-prompt avec:
- 10 sections,
- 5 rÃ´les,
- 3 chain-of-thought imbriquÃ©s,
- des exemples Ã  foison,

â€¦noie le signal clÃ© dans beaucoup de bruit. Un paragraphe trÃ¨s clair rÃ©duit la distance sÃ©mantique entre ta demande et lâ€™information pertinenteã€turn6fetch0ã€‘.

### 2) Moins de distracteurs = moins dâ€™erreurs

- PromptLayer insiste sur le fait que mÃªme quelques â€œdistracteursâ€ (info plausible mais fausse ou hors sujet) suffisent Ã  faire chuter la performance, et que 4 distracteurs peuvent la â€œtuerâ€ complÃ¨tementã€turn6fetch0ã€‘.
- Un prompt trÃ¨s structurÃ© qui explique plein de choses impose au modÃ¨le de suivre une â€œnarrativeâ€ de prompt, qui peut nuire Ã  sa capacitÃ© Ã  retrouver lâ€™information prÃ©cise.

### 3) Le modÃ¨le a dÃ©jÃ  beaucoup de connaissances

- Dans les docs de Skills, Anthropic dit explicitement: â€œDefault assumption: Claude is already very smartâ€ â€“ il faut nâ€™ajouter que ce que le modÃ¨le ne sait pas dÃ©jÃ ã€turn9fetch1ã€‘.
- Beaucoup de â€œbest practicesâ€ complexes redÃ©couvrent lâ€™eau chaude: demander explicitement des choses que le modÃ¨le ferait de toute faÃ§on si tu lui donnais juste un cadre clair (objectif + contraintes).

### 4) Moins de contraintes = plus de libertÃ© dâ€™adaptation

- Des Ã©tudes indiquent que des prompts zÃ©ro-shot trÃ¨s simples peuvent outperformer des prompts few-shot plus complexes dans certains scÃ©nariosã€turn4search1ã€‘.
- Si tu micro-manages chaque Ã©tape, tu limites la capacitÃ© du modÃ¨le Ã :
  - choisir la bonne stratÃ©gie,
  - adapter le style Ã  la base de code,
  - optimiser pour la maintenabilitÃ© ou la performance du code.

### 5) Sur les agents: une boucle simple vs un graphe complexe

- Plusieurs praticiens (et des posts relayÃ©s en sÃ©rie) expliquent que Claude Code fonctionne si bien parce quâ€™il repose sur une seule boucle simple, avec au plus une branche, et une todo-list pour garder le focus, plutÃ´t quâ€™un graphe de multi-agents compliquÃ©ã€turn7fetch1ã€‘ã€turn8search14ã€‘.
- Chaque couche supplÃ©mentaire (agents sur agents, hooks sur hooks, skills sur skills) rend:
  - le debugging plus dur,
  - lâ€™observabilitÃ© plus faible,
  - lâ€™itÃ©ration plus lente.

Dâ€™oÃ¹ le sentiment intuitif mais trÃ¨s rÃ©el: â€œje pose une question simple dans Claude Code / OpenCode / Antigravity, et Ã§a marche mieux que mon mega-prompt de 40 lignesâ€.

---

## 3. Ce que Ã§a implique pour le prompt engineering

En pratique, un â€œprompt engineering performance-firstâ€ ressemble plus Ã  de la chirurgie quâ€™Ã  de la littÃ©rature.

Voici un cycle de travail simple pour maximiser la performance tout en gardant la complexitÃ© basse:

```mermaid
flowchart LR
  A[Objectif rÃ©el] --> B[Paragraphe court - intention + contexte + contraintes]
  B --> C[Observer: erreurs et outputs]
  C --> D{Trop d erreurs ?}
  D -- Oui --> E[Ajouter 1 info ciblÃ©e - exemple ou contrainte prÃ©cise]
  E --> B
  D -- Non --> F[Facultatif - capturer en commande ou skill]
```

Les principes concrets:

1) Commencer hyper-simple
   - 1 phrase dâ€™objectif:
     - â€œAjoute lâ€™endpoint POST /users avec validation email/password, erreurs 400/409/422, sans changer le schÃ©ma existant.â€
   - 1 phrase de contexte:
     - â€œLe projet est une API REST FastAPI avec SQLAlchemy, repo dans ./app.â€
   - 1â€“2 contraintes:
     - â€œNe touche pas aux migrations, utilise les modÃ¨les existants.â€

2) Ajouter de lâ€™information uniquement quand tu observes un problÃ¨me
   - Si le modÃ¨le:
     - ne respecte pas une convention â†’ ajoute 1 ligne prÃ©cisant la convention.
     - se perd dans les fichiers â†’ limite via le contexte (ex: â€œregarde uniquement ./app/api/v1/â€¦â€).
   - Ã€ chaque ajout, tu te poses la question des Skills: â€œEst-ce que Claude a vraiment besoin de ce texte pour rÃ©ussir ?â€ã€turn9fetch1ã€‘.

3) Utiliser le dÃ©coupage naturel du problÃ¨me plutÃ´t que des frameworks de prompting
   - GitHub recommande de â€œcasser les tÃ¢ches complexes en tÃ¢ches simplesâ€ et de les enchainer, plutÃ´t que de tout mettre dans un seul promptã€turn13fetch0ã€‘.
   - Pour un feature complexe:
     1) â€œGÃ©nÃ¨re les modÃ¨les DTO + Pydantic.â€
     2) â€œGÃ©nÃ¨re la route FastAPI avec ces modÃ¨les.â€
     3) â€œGÃ©nÃ¨re les tests unitaires pour cette route.â€
   - Chaque sous-taÃ¢che a un prompt de 2â€“3 lignes.

4) Faire confiance aux capacitÃ©s de reasoning du modÃ¨le
   - Anthropic recommande dâ€™utiliser le â€œextended thinkingâ€ ou CoT sur les tÃ¢ches qui nÃ©cessitait vraiment du raisonnement, et de le faire aprÃ¨s avoir rÃ©duit le contexte (par exemple aprÃ¨s un RAG propre)ã€turn6fetch0ã€‘.
   - Ne forcer pas un â€œreasoningâ€ lourd pour des tÃ¢ches simples (boilerplate, refactorings localisÃ©s) â€“ Ã§a ralentit sans gain.

5) GÃ©rer le contexte comme un budget, pas comme un ocÃ©an infini
   - Anthropic: la bonne ingÃ©nierie du contexte, câ€™est trouver â€œle plus petit ensemble de tokens Ã  haut signal possibleâ€ pour atteindre lâ€™outcome vouluã€turn9fetch0ã€‘.
   - PromptLayer: pour du RAG, il faut â€œretriever moins de 1000 tokens de haute similaritÃ©â€ et Ã©liminer les distracteurs avant quâ€™ils nâ€™empoisonnent le contexteã€turn6fetch0ã€‘.
  - En pratique, avec Claude Code / OpenCode:
    - Ã‰vite de lui jeter 20 fichiers â€œau cas oÃ¹â€.
    - Cible 3â€“5 fichiers max pour la tÃ¢che courante, et laisse lâ€™outil naviguer si besoin.

6) Suivre un pattern de prompt simple, rÃ©utilisable
   Un pattern trÃ¨s efficace pour le code:
   - Je veux [outcome prÃ©cis]
   - Contexte: [stack, dossier, module, contraintes]
   - Contraintes:
     - [ne pas changer X, respecter Y, prioriser Z]
   - Output souhaitÃ©:
     - [un patch, un plan, ou un fichier]

   Câ€™est plus proche dâ€™un ticket de dev humain que dâ€™un â€œframeworkâ€ thÃ©orique.

---

## 4. Ce que Ã§a implique pour les Skills et les Agents

Les docs officielles et les retours de communautÃ© disent la mÃªme chose: Skills et Agents doivent Ãªtre des â€œaccÃ©lÃ©rateurs de contextes et de comportementsâ€, pas des scripts gÃ©ants.

### A) Skills: viser la compaction et le ciblage, pas le tutoriel

- Claude Skills:
  - â€œConcise is keyâ€ â€“ chaque token dans le SKILL.md partage la fenÃªtre de contexte avec le prompt, lâ€™historique et les autres Skillsã€turn9fetch1ã€‘.
  - Anthropic recommande de supposer que Claude est dÃ©jÃ  trÃ¨s smart, et de nâ€™ajouter que ce quâ€™il ne sait pasã€turn9fetch1ã€‘.
  - Ils donnent un exemple â€œbonâ€ de ~50 tokens (juste le snippet Python avec pdfplumber) vs un â€œmauvaisâ€ de ~150 tokens avec explications inutiles sur ce quâ€™est un PDFã€turn9fetch1ã€‘.
- OpenCode Skills:
  - Docs: â€œAgent skills let OpenCode discover reusable instructions from your repo or home directoryâ€ â€“ elles sont chargÃ©es Ã  la demande via le tool skillã€turn9fetch2ã€‘.
  - Les descriptions sont limitÃ©es Ã  1â€“1024 caractÃ¨res et doivent rester â€œspÃ©cifiques pour que lâ€™agent choisisse correctementâ€ã€turn9fetch2ã€‘.
  - Lâ€™exemple de skill git-release est ultra-simple: quelques lignes qui disent:
    - ce que le skill fait (draft release notes, bump version, commande gh release),
    - quand lâ€™utiliser (prÃ©paration de tagged release),
    - comment se comporter (poser des questions si le schÃ©ma de version nâ€™est pas clair)ã€turn9fetch2ã€‘.

En pratique: un bon Skill = une procÃ©dure ultra-condensÃ©e + quand lâ€™utiliser.

Exemple de skill â€œrefactor-typescript-strictâ€ (compatible Claude & OpenCode):

```yaml
---
name: refactor-typescript-strict
description: Refactor existing TS files toward strict mode safety
---

## What I do
- Enable `strict: true` if not already present, minimally.
- Add explicit return types where `noImplicitAny` would break.
- Replace `any` by `unknown` + guards when possible.
- Update tests to cover new type errors.

## When to use me
Use when you have a working TS module and want to make it safer without changing behavior.
Ask if breaking changes are allowed before touching exported APIs.
```

Câ€™est trÃ¨s court, mais trÃ¨s dense en contraintes opÃ©rationnelles.

### B) Agents: un agent = une responsabilitÃ© claire, un contexte sÃ©parÃ© si besoin

- Claude Code:
  - Des dÃ©veloppeurs ont crÃ©Ã© des sub-agents pour encapsuler des opÃ©rations lourdes (analyse de gros logs, scans de code multi-fichiers) et renvoyer seulement un rÃ©sumÃ©, ce qui rÃ©duit la â€œcontext pollutionâ€ de 95â€“98%ã€turn8search0ã€‘.
  - Lâ€™idÃ©e nâ€™est pas dâ€™ajouter des agents â€œparce que câ€™est coolâ€, mais dâ€™isoler un contexte pour ne pas polluer la boucle principale.
- Anthropic â€œcontext engineeringâ€:
  - Lâ€™art de la compaction est de dÃ©cider quoi garder (dÃ©cisions dâ€™architecture, bugs ouverts) vs quoi jeter (outputs dâ€™outils, messages redondants)ã€turn9fetch0ã€‘.
- Docs Copilot:
  - Ils prÃ©conisent dâ€™utiliser des agents pour des workflows bien dÃ©finis (ex: code review) plutÃ´t que de crÃ©er un â€œsuper agentâ€ multi-purposeã€turn11fetch0ã€‘ã€turn11fetch1ã€‘.

Bon pattern pour les agents:

- Un agent principal = â€œdev leadâ€: boucle principale, gestion de la todo-list, appels aux autres agents/skills si besoin.
- 1â€“3 sub-agents trÃ¨s ciblÃ©s:
  - test-diagnostic,
  - perf-profiler,
  - release-manager,
  - etc.
- Chaque sub-agent:
  - a un prompt systÃ¨me trÃ¨s court,
  - reÃ§oit un contexte limitÃ© (ex: un sous-ensemble de fichiers, un rapport de test),
  - renvoie une rÃ©ponse structurÃ©e courte (summary + plan dâ€™actions).

OpenCode formalise la distinction:
- Commands = saved prompts (le â€œquoiâ€),
- Agents = personas avec permissions (le â€œquiâ€),
- Skills = docs de connaissance chargÃ©es Ã  la demande (le â€œsavoirâ€)ã€turn8search10ã€‘.

Donc:
- Tu nâ€™as pas besoin dâ€™un nouvel agent pour chaque prompt.
- Un bon agent + quelques skills + quelques commands = dÃ©jÃ  trÃ¨s puissant.

### C) Hooks, MCP, plugins: lÃ  encore, less is more

Claude Code/OpenCode/Copilot intÃ¨grent:
- Hooks (lifecycle des sessions, prÃ©/post actions),
- MCP (Model Context Protocol) pour connecter des outils externes,
- Plugins (OpenCode).

Les docs Claude Code sur MCP montrent que tu peux brancher Ã©normÃ©ment de services (GitHub, Jira, Sentry, Slack, bases de donnÃ©esâ€¦) et les invoquer par des requÃªtes naturelles simplesã€turn5fetch0ã€‘.

Les risques si tu ne penses pas â€œperformanceâ€:
- MCP + Skills + Hooks + Sub-agents + 3 providers de modÃ¨les = surface Ã©norme.
- Tu passes plus de temps Ã  configurer lâ€™usine quâ€™Ã  produire du code.

Approche simple:
- Hooks:
  - Uniquement pour ce qui est:
    - rÃ©pÃ©titif (prÃ©-commit checks, formatting),
    - ou critique (synchronisation avec un ticket).
- MCP:
  - 3â€“5 intÃ©grations vraiment utiles (ex: GitHub + Sentry + une BDD + Slack).
- Plugins:
  - Uniquement ceux qui rÃ©duisent la charge mentale (formatage, nettoyage de tables, etc.), pas ceux qui rajoutent des vues ou abstractions.

---

## 5. Toolkit focus & mode opÃ©ratoire avec Claude Code / OpenCode (et Antigravity)

### A) Principe de base du â€œtoolkit focusâ€

Au lieu de 1001 combinaisons, je garde:

- 1 prompt principal â€œpar dÃ©fautâ€ pour chaque type de tÃ¢che (feature, bugfix, refactor, test, doc).
- 3â€“6 Skills ultra-courts par projet (stack, conventions spÃ©cifiques, release, etc.).
- 1 agent principal + 2â€“3 sub-agents au max.
- 3â€“5 intÃ©grations MCP rÃ©ellement utilisÃ©es.
- Quelques hooks automatisant les actions Ã©videntes (format, lint, test).

Tout le reste est considÃ©rÃ© comme bruit jusquâ€™Ã  preuve du contraire.

### B) Avec Claude Code

Objectif: utiliser Claude Code comme un â€œdev leadâ€ Ã  une boucle simple.

1) Prompt de base ultra-simple dans le terminal:

   - Feature:
     - â€œAjoute la feature dÃ©crite dans ce ticket JIRA: ENG-4521, en respectant nos conventions FastAPI/SQLAlchemy et sans changer le schÃ©ma de la base.â€
   - Bug:
     - â€œCorrige ce bug: [erreur stacktrace], en modifiant le moins de code possible, en ajoutant un test unitaire.â€
   - Refactor:
     - â€œRefactor ce module pour utiliser notre nouvelle couche de service, sans changer les signatures publiques.â€

   Anthropic insiste dans lâ€™overview de Claude Code sur lâ€™importance de donner une description en langage naturel et de laisser lâ€™outil planifier + Ã©crire + validerã€turn1fetch0ã€‘.

2) Utiliser Skills comme â€œcheat sheetsâ€ ultra-lÃ©gers

   - Structure:
     - `SKILL.md` avec YAML frontmatter (name, description) et deux sections: â€œWhat I doâ€, â€œWhen to use meâ€ã€turn9fetch2ã€‘.
   - Exemple skill FastAPI pour ton projet:

     ```yaml
     ---
     name: fastapi-crud-conventions
     description: Follow our FastAPI CRUD patterns
     ---

     ## What I do
     - Use `router.prefix` for resource routes.
     - Expose only `/items/{id}` and `/items/` endpoints unless explicit requirement.
     - Always validate with Pydantic V2 models.
     - Return 404 if not found, 422 for validation errors, 400 for bad requests.

     ## When to use me
     Use for any new CRUD route or DTO.
     Ask if pagination/search is required before implementing.
     ```

   - Claude Code/OpenCode chargeront ce skill seulement quand ils en ont besoinã€turn9fetch2ã€‘.

3) Sub-agents seulement pour la gestion de contexte

   - Exemples:
     - `@test-diagnostic`:
       - ReÃ§oit la sortie des tests, renvoie un plan de fixes.
     - `@release-notes`:
       - GÃ©nÃ¨re release notes et changelog Ã  partir de la liste de PR.
   - Ethan Houseworth montre que de tels sub-agents peuvent rÃ©duire la context pollution de 95â€“98% en isolant les opÃ©rations lourdes et en ne renvoyant que le rÃ©sumÃ©ã€turn8search0ã€‘.

4) MCP: quelques outils trÃ¨s ciblÃ©s

   - Exemple de setup minimal:
     - GitHub (PR, issues),
     - Sentry (erreurs),
     - une BDD (ex: PostgreSQL),
     - Slack (notifications).
   - Ensuite, tu invoques avec des requÃªtes simples, comme le montre la doc MCP:
     - â€œImplÃ©mente la feature dÃ©crite dans JIRA ENG-4521 et crÃ©e une PR GitHub.â€
     - â€œVÃ©rifie Sentry et Statsig pour lâ€™utilisation de la feature ENG-4521.â€ã€turn5fetch0ã€‘

5) Hooks pour les tÃ¢ches triviales

   - PrÃ©-commit:
     - Lint + format + tests rapides.
   - Post-modification:
     - Mettre Ã  jour un fichier `CHANGELOG.md` si le skill de release est activÃ©.

Tout Ã§a se configure dans `claude.json` / `mcp.json` / hooks, mais lâ€™objectif est dâ€™oublier la configuration pendant que tu codes.

### C) Avec OpenCode

OpenCode est un agent de code open-source, avec support de multiples modÃ¨les, LSP, sessions parallÃ¨les, skills compatibles Claude, plugins MCP, etc.ã€turn5fetch1ã€‘ã€turn5fetch2ã€‘.

1) Utiliser le mÃªme mental modÃ¨le Claude Code:

   - 1 agent principal = ton AI lead.
   - 2â€“3 skills critiques par projet.
   - Quelques commands (saved prompts) pour:
     - â€œgÃ©nÃ©rer testsâ€,
     - â€œrefactor module Xâ€,
     - â€œcrÃ©er PR descriptionâ€.

2) Skills OpenCode = mÃªme principe que Claude Skills

   - Tu crÃ©es:
     - `.opencode/skills/<name>/SKILL.md`
     - ou `~/.config/opencode/skills/<name>/SKILL.md`ã€turn9fetch2ã€‘.
   - OpenCode liste les skills dans le tool skill et lâ€™agent appelle skill({ name: "..." }) quand besoinã€turn9fetch2ã€‘.
   - Tu peux rÃ©utiliser les mÃªmes skills Claude, car OpenCode supporte aussi `.claude/skills/*/SKILL.md`ã€turn9fetch2ã€‘.

3) Agents OpenCode

   - Docs:
     - `Agents` = personas avec permissions,
     - `Commands` = prompts rÃ©utilisables,
     - `Skills` = docs de connaissance chargÃ©es Ã  la demandeã€turn5fetch2ã€‘ã€turn8search10ã€‘.
   - Bon setup:
     - Un agent â€œdevâ€ avec lecture/Ã©criture dans le code repo.
     - Un agent â€œopsâ€ avec accÃ¨s limitÃ© (CI/CD, scripts).
     - Quelques commands simples liÃ©es Ã  ces agents:
       - `opencode run-command test-all --agent dev`
       - `opencode run-command perf-check --agent ops`

4) Plugins OpenCode trÃ¨s sÃ©lectifs

   - Dans lâ€™Ã©cosystÃ¨me, tu as par ex:
     - `opencode-antigravity-auth` pour utiliser les modÃ¨les Antigravity gratuitsã€turn5fetch2ã€‘,
     - `opencode-dynamic-context-pruning` pour optimiser lâ€™usage des tokens en supprimant les outputs dâ€™outils obsolÃ¨tesã€turn5fetch2ã€‘,
     - `opencode-skillful` pour lazy-load des skills Ã  la demandeã€turn5fetch2ã€‘.
   - Pour une approche â€œless is moreâ€, je garderais:
     - un plugin dâ€™auth modÃ¨le (par ex Antigravity ou Zen),
     - un plugin de pruning de contexte,
     - au plus 1â€“2 plugins mÃ©tier (tests, notifications).

### D) Antigravity / Google AI Antigravity IDE

Antigravity pousse le concept de â€œvibe codingâ€: dÃ©crire ce que tu veux en langage naturel, laisser lâ€™agent planifier et exÃ©cuter (Ã©diteur, terminal, browser) sans micro-managerã€turn10search3ã€‘.

- Les retours trÃ¨s positifs disent que lâ€™approche â€œje dis juste ce que je veuxâ€ marche trÃ¨s bien quand:
  - la description est courte mais prÃ©cise (outcome + contraintes),
  - on laisse lâ€™orchestration Ã  lâ€™agent plutÃ´t que dâ€™essayer de contrÃ´ler chaque Ã©tapeã€turn10search2ã€‘.
- Les retours mitigÃ©s montrent des limites quand:
  - on enchaÃ®ne trop dâ€™agents en parallÃ¨le,
  - on oublie de tester les rÃ©sultats,
  - on surcharge de contexte lâ€™IDE.

ConsÃ©quence â€œless is moreâ€:
- Un prompt simple dans Antigravity (â€œRefactor cette API pour suivre notre pattern Xâ€) fonctionne mieux quâ€™un plan Ã©tape par Ã©tape ultra-dÃ©taillÃ© dans le prompt.
- Tu gardes la sophistication dans la stack (MCP, plugins), mais tu gardes le contrÃ´le humain sur la boucle (review, tests).

---

## Conclusion: de â€œprompt engineerâ€ Ã  â€œcontext engineerâ€ et â€œflow engineerâ€

Si je rÃ©sume ton changement de posture:

- Avant:
  - Passer beaucoup de temps Ã  apprendre des primitives (prompts, skills, subagents, tools, MCP, hooks).
  - Construire des prompts et des architectures complexes.
- Maintenant:
  - DÃ©sapprendre lâ€™over-engineering.
  - Penser performance:
    - minimal prompt haute-densitÃ©,
    - minimal nombre dâ€™agents/skills,
    - maximal signal dans le contexte.

La recherche et la communautÃ© disent aujourdâ€™hui:

- Ce qui compte, ce nâ€™est pas la longueur du prompt, mais sa densitÃ© dâ€™information et sa clartÃ©ã€turn4search0ã€‘ã€turn9fetch0ã€‘.
- Les compÃ©tences-clÃ©s deviennent:
  - â€œcontext engineeringâ€: choisir le plus petit ensemble de tokens pertinent pour chaque tÃ¢cheã€turn9fetch0ã€‘,
  - â€œflow engineeringâ€: dÃ©couper un problÃ¨me complexe en micro-tÃ¢ches simples avec une boucle dâ€™agent claire, plutÃ´t que de construire un graphe de multi-agents touffuã€turn4search7ã€‘ã€turn7fetch1ã€‘.

ConcrÃ¨tement, ton nouveau mode opÃ©ratoire pourrait Ãªtre:

- Prompt:
  - 1 paragraphe par tÃ¢che.
  - ItÃ©rer: +1 ligne si problÃ¨me observÃ©.
- Skills:
  - 3â€“6 skills ultra-courts par projet.
  - Pattern â€œWhat I do / When to use meâ€.
- Agents:
  - 1 agent principal + 2â€“3 sub-agents de contexte.
  - Chaque agent = une responsabilitÃ© trÃ¨s claire.
- Outils:
  - Claude Code comme boucle principale (dev lead).
  - OpenCode comme alternative open-source avec le mÃªme mindset.
  - MCP/plugins limitÃ©s Ã  ce qui supprime de la friction rÃ©elle.
- Workflow:
  - Toujours: intention courte â†’ observer â†’ ajuster.
  - Capturer en Skill/commande uniquement ce qui se rÃ©pÃ¨te et qui apporte vraiment de la valeur.

Câ€™est ce â€œless is moreâ€, Ã©tayÃ© par la recherche et la pratique, qui te fait passer de la technicitÃ© Ã  la performance.

--- tokens: 6216 out / 38602 in | time: 2026-02-04 23:24:25
